replaces movaps instructions with movups in order to workaround the sse
issue in DOS DXE builds. reason for the crashes is yet to be determined.

diff --git sage/drivers/glide/x86/sse_vbtmp.asm sage/drivers/glide/x86/sse_vbtmp.asm
index f25354a..c9abaf0 100644
--- sage/drivers/glide/x86/sse_vbtmp.asm
+++ sage/drivers/glide/x86/sse_vbtmp.asm
@@ -13,11 +13,11 @@ proc	TAG(sse_emitvertices)
     ; Viewport transformation
 	mov	ebx, [tnl_vb + TNL_VB_CLIPMASK]
 	mov	esi, [tnl_vb + TNL_VB_NDC]
-	movaps	xmm7, [i_xxx0]
+	movups	xmm7, [i_xxx0]
 	movss	xmm2, [ctx_mx_viewport + MAT_MAT + 00 * 4]
 	movss	xmm1, [ctx_mx_viewport + MAT_MAT + 05 * 4]
 	movss	xmm3, [ctx_mx_viewport + MAT_MAT + 10 * 4]
-	movaps	xmm5, [f_0010]
+	movups	xmm5, [f_0010]
 	shufps	xmm2, xmm1, SHUF(X, Y, X, Y)
 	orps	xmm3, xmm5
 	movups	xmm4, [ctx_mx_viewport + MAT_MAT + 12 * 4]
@@ -29,10 +29,10 @@ proc	TAG(sse_emitvertices)
 	mov	dword [edi + GR_VERTEX_OOW_OFFSET], 0x3f800000
 	test	al, al
 	jnz	.1
-	movaps	xmm0, [esi]
+	movups	xmm0, [esi]
 	mulps	xmm0, xmm2
 	addps	xmm0, xmm4
-	movaps	[edi + GR_VERTEX_X_OFFSET], xmm0
+	movups	[edi + GR_VERTEX_X_OFFSET], xmm0
 	align	16
     .1:
 	inc	ebx
@@ -50,11 +50,11 @@ proc	TAG(sse_emitvertices)
 	mov	esi, [tnl_vb + TNL_VB_COLOR0_DATA]
 	mov	ebx, [tnl_vb + TNL_VB_COLOR0_STRIDE]
 	xorps	xmm6, xmm6
-	movaps	xmm7, [f_bbbb]
+	movups	xmm7, [f_bbbb]
 	shl	ebx, 4
 	align	16
     .2:
-	movaps	xmm0, [esi]
+	movups	xmm0, [esi]
 	mulps	xmm0, xmm7
 %if FX_PACKEDCOLOR
 	shufps	xmm0, xmm0, SHUF(Z, Y, X, W)
@@ -68,7 +68,7 @@ proc	TAG(sse_emitvertices)
 %else  ; !FX_PACKEDCOLOR
 	minps	xmm0, xmm7
 	maxps	xmm0, xmm6
-	movaps	[edi + GR_VERTEX_RGB_OFFSET], xmm0
+	movups	[edi + GR_VERTEX_RGB_OFFSET], xmm0
 %endif ; !FX_PACKEDCOLOR
 	add	esi, ebx
 	add	edi, sizeof_GrVertex
@@ -100,7 +100,7 @@ proc	TAG(sse_emitvertices)
 	align	16
     .3:
 	movss	xmm6, [edi + GR_VERTEX_OOW_OFFSET]
-	movaps	xmm0, [esi]
+	movups	xmm0, [esi]
 	shufps	xmm6, xmm6, SHUF(X, X, X, X)
 	mulps	xmm0, xmm7
 	mulps	xmm0, xmm6
@@ -140,7 +140,7 @@ proc	TAG(sse_emitvertices)
 	align	16
     .4:
 	movss	xmm6, [edi + GR_VERTEX_OOW_OFFSET]
-	movaps	xmm0, [esi]
+	movups	xmm0, [esi]
 	shufps	xmm6, xmm6, SHUF(X, X, X, X)
 	mulps	xmm0, xmm7
 	mulps	xmm0, xmm6
@@ -164,11 +164,11 @@ proc	TAG(sse_emitvertices)
 	mov	esi, [tnl_vb + TNL_VB_COLOR1_DATA]
 	mov	ebx, [tnl_vb + TNL_VB_COLOR1_STRIDE]
 	xorps	xmm6, xmm6
-	movaps	xmm7, [f_bbbb]
+	movups	xmm7, [f_bbbb]
 	shl	ebx, 4
 	align	16
     .5:
-	movaps	xmm0, [esi]
+	movups	xmm0, [esi]
 	mulps	xmm0, xmm7
 %if FX_PACKEDCOLOR
 	shufps	xmm0, xmm0, SHUF(Z, Y, X, W)
@@ -182,7 +182,7 @@ proc	TAG(sse_emitvertices)
 %else  ; !FX_PACKEDCOLOR
 	minps	xmm0, xmm7
 	maxps	xmm0, xmm6
-	movaps	[edi + GR_VERTEX_SPEC_OFFSET], xmm0
+	movups	[edi + GR_VERTEX_SPEC_OFFSET], xmm0
 %endif ; !FX_PACKEDCOLOR
 	add	esi, ebx
 	add	edi, sizeof_GrVertex
diff --git sage/x86/sse_clip.asm sage/x86/sse_clip.asm
index 6f2e667..b54c90a 100644
--- sage/x86/sse_clip.asm
+++ sage/x86/sse_clip.asm
@@ -52,7 +52,7 @@ proc	sse_clipmask
 	jz	.next_k
 	align	16
     .j_loop:
-	movaps	xmm2, [esi]
+	movups	xmm2, [esi]
 	rcpps	xmm0, xmm2
 	push	ebp
 
@@ -78,7 +78,7 @@ proc	sse_clipmask
 	mov	cl, [x86_clip_lut + ecx]
 
 	pop	ebp
-	movaps	xmm1, xmm2
+	movups	xmm1, xmm2
 	mov	[ebx], cl
 	or	eax, ecx
 	test	ecx, ecx
@@ -87,7 +87,7 @@ proc	sse_clipmask
 	shufps	xmm0, xmm0, SHUF(W, W, W, W)
 	mulps	xmm2, xmm0
 	movt	xmm2, xmm0
-	movaps	[ebp], xmm2
+	movups	[ebp], xmm2
 	align	16
     .next_j:
 	add	esi, 16
diff --git sage/x86/sse_mat.asm sage/x86/sse_mat.asm
index b067a70..146c12f 100644
--- sage/x86/sse_mat.asm
+++ sage/x86/sse_mat.asm
@@ -73,13 +73,13 @@ proc	matrix_mul_vec4_batch_sse
 	mov	ecx, [esp+16]
 	align	16
     .0:
-	movaps	xmm0, [eax]
+	movups	xmm0, [eax]
 	prefetchnta	[eax+0x30]
-	movaps	xmm1, xmm0
+	movups	xmm1, xmm0
 	add	eax, 16
-	movaps	xmm2, xmm0
+	movups	xmm2, xmm0
 	add	edx, 16
-	movaps	xmm3, xmm0
+	movups	xmm3, xmm0
 	prefetchnta	[edx+0x20]
 	shufps	xmm0, xmm0, SHUF(X, X, X, X)
 	shufps	xmm1, xmm1, SHUF(Y, Y, Y, Y)
@@ -92,7 +92,7 @@ proc	matrix_mul_vec4_batch_sse
 	addps	xmm1, xmm0
 	addps	xmm2, xmm3
 	addps	xmm1, xmm2
-	movaps	[edx-16], xmm1
+	movups	[edx-16], xmm1
 	dec	ecx
 	jnz	.0
 	ret
@@ -111,11 +111,11 @@ proc	matrix_mul_vec3_batch_sse
 	mov	ecx, [esp+16]
 	align	16
     .0:
-	movaps	xmm0, [eax]
+	movups	xmm0, [eax]
 	prefetchnta	[eax+0x30]
-	movaps	xmm1, xmm0
+	movups	xmm1, xmm0
 	add	eax, 16
-	movaps	xmm2, xmm0
+	movups	xmm2, xmm0
 	add	edx, 16
 	prefetchnta	[edx+0x20]
 	shufps	xmm0, xmm0, SHUF(X, X, X, X)
@@ -127,7 +127,7 @@ proc	matrix_mul_vec3_batch_sse
 	addps	xmm1, xmm0
 	addps	xmm2, xmm7
 	addps	xmm1, xmm2
-	movaps	[edx-16], xmm1
+	movups	[edx-16], xmm1
 	dec	ecx
 	jnz	.0
 	ret
diff --git sage/x86/sse_misc.asm sage/x86/sse_misc.asm
index b30d822..cedb140 100644
--- sage/x86/sse_misc.asm
+++ sage/x86/sse_misc.asm
@@ -31,10 +31,10 @@ proc	sse_calc_veyn4
 	mov	ecx, [tnl_vb + TNL_VB_LEN]
 	align	16
     .0:
-	movaps	xmm0, [eax]
-	movaps	xmm1, xmm0
-	movaps	xmm2, xmm0
-	movaps	xmm3, xmm0
+	movups	xmm0, [eax]
+	movups	xmm1, xmm0
+	movups	xmm2, xmm0
+	movups	xmm3, xmm0
 	shufps	xmm0, xmm0, SHUF(X, X, X, X)
 	shufps	xmm1, xmm1, SHUF(Y, Y, Y, Y)
 	shufps	xmm2, xmm2, SHUF(Z, Z, Z, Z)
@@ -46,19 +46,19 @@ proc	sse_calc_veyn4
 	addps	xmm1, xmm0
 	addps	xmm2, xmm3
 	addps	xmm1, xmm2
-	movaps	[edx], xmm1			;  x  |  y  |  z  |  w
+	movups	[edx], xmm1			;  x  |  y  |  z  |  w
 
-	movaps	xmm0, xmm1			;  x  |  y  |  z  |  w
+	movups	xmm0, xmm1			;  x  |  y  |  z  |  w
 	mulps	xmm1, xmm1			; x*x | y*y | z*z | w*w
-	movaps	xmm2, xmm1
-	movaps	xmm3, xmm1
+	movups	xmm2, xmm1
+	movups	xmm3, xmm1
 	shufps	xmm2, xmm2, SHUF(Y, Z, X, W)	; y*y | z*z | x*x | w*w
 	shufps	xmm3, xmm3, SHUF(Z, X, Y, W)	; z*z | x*x | y*y | w*w
 	addps	xmm1, xmm2
 	addps	xmm1, xmm3
 	rsqrtps	xmm1, xmm1
 	mulps	xmm0, xmm1
-	movaps	[edi], xmm0
+	movups	[edi], xmm0
 
 	add	eax, 16
 	add	edx, 16
@@ -85,9 +85,9 @@ proc	sse_calc_veyn3
 	mov	ecx, [tnl_vb + TNL_VB_LEN]
 	align	16
     .0:
-	movaps	xmm0, [eax]
-	movaps	xmm1, xmm0
-	movaps	xmm2, xmm0
+	movups	xmm0, [eax]
+	movups	xmm1, xmm0
+	movups	xmm2, xmm0
 	shufps	xmm0, xmm0, SHUF(X, X, X, X)
 	shufps	xmm1, xmm1, SHUF(Y, Y, Y, Y)
 	shufps	xmm2, xmm2, SHUF(Z, Z, Z, Z)
@@ -97,19 +97,19 @@ proc	sse_calc_veyn3
 	addps	xmm1, xmm0
 	addps	xmm2, xmm7
 	addps	xmm1, xmm2
-	movaps	[edx], xmm1			;  x  |  y  |  z  |  w
+	movups	[edx], xmm1			;  x  |  y  |  z  |  w
 
-	movaps	xmm0, xmm1			;  x  |  y  |  z  |  w
+	movups	xmm0, xmm1			;  x  |  y  |  z  |  w
 	mulps	xmm1, xmm1			; x*x | y*y | z*z | w*w
-	movaps	xmm2, xmm1
-	movaps	xmm3, xmm1
+	movups	xmm2, xmm1
+	movups	xmm3, xmm1
 	shufps	xmm2, xmm2, SHUF(Y, Z, X, W)	; y*y | z*z | x*x | w*w
 	shufps	xmm3, xmm3, SHUF(Z, X, Y, W)	; z*z | x*x | y*y | w*w
 	addps	xmm1, xmm2
 	addps	xmm1, xmm3
 	rsqrtps	xmm1, xmm1
 	mulps	xmm0, xmm1
-	movaps	[edi], xmm0
+	movups	[edi], xmm0
 
 	add	eax, 16
 	add	edx, 16
@@ -138,11 +138,11 @@ proc	sse_calc_neye
 %if 0
 	align	16
     .0:
-	movaps	xmm0, [eax]
+	movups	xmm0, [eax]
 	prefetchnta	[eax+0x30]
-	movaps	xmm1, xmm0
+	movups	xmm1, xmm0
 	add	eax, 16
-	movaps	xmm2, xmm0
+	movups	xmm2, xmm0
 	add	edx, 16
 	prefetchnta	[edx+0x20]
 	shufps	xmm0, xmm0, SHUF(X, X, X, X)
@@ -153,7 +153,7 @@ proc	sse_calc_neye
 	mulps	xmm2, xmm6
 	addps	xmm1, xmm0
 	addps	xmm1, xmm2
-	movaps	[edx-16], xmm1
+	movups	[edx-16], xmm1
 	dec	ecx
 	jnz	.0
 %else
@@ -165,10 +165,10 @@ proc	sse_calc_neye
     .0:
 	test	edi, TNL_NORMAL_BIT
 	jz	.1
-	movaps	xmm0, [eax]
+	movups	xmm0, [eax]
 	prefetchnta	[eax+0x30]
-	movaps	xmm1, xmm0
-	movaps	xmm2, xmm0
+	movups	xmm1, xmm0
+	movups	xmm2, xmm0
 	prefetchnta	[edx+0x30]
 	shufps	xmm0, xmm0, SHUF(X, X, X, X)
 	shufps	xmm1, xmm1, SHUF(Y, Y, Y, Y)
@@ -180,7 +180,7 @@ proc	sse_calc_neye
 	addps	xmm1, xmm2
 	align	16
     .1:
-	movaps	[edx], xmm1
+	movups	[edx], xmm1
 	add	esi, 4
 	add	eax, 16
 	add	edx, 16
@@ -193,9 +193,9 @@ proc	sse_calc_neye
 	ret
 	align	16
     .2:
-	movaps	xmm0, [eax]
-	movaps	xmm1, xmm0
-	movaps	xmm2, xmm0
+	movups	xmm0, [eax]
+	movups	xmm1, xmm0
+	movups	xmm2, xmm0
 	shufps	xmm0, xmm0, SHUF(X, X, X, X)
 	shufps	xmm1, xmm1, SHUF(Y, Y, Y, Y)
 	shufps	xmm2, xmm2, SHUF(Z, Z, Z, Z)
@@ -206,7 +206,7 @@ proc	sse_calc_neye
 	addps	xmm1, xmm2
 	align	16
     .3:
-	movaps	[edx], xmm1
+	movups	[edx], xmm1
 	add	edx, 16
 	dec	ecx
 	jnz	.3
@@ -227,16 +227,16 @@ proc	sse_reflect
 	mov	eax, [tnl_vb + TNL_VB_VEYN]
 	mov	ecx, [tnl_vb + TNL_VB_NEYE]
 	mov	edx, [tnl_vb + TNL_VB_REFL]
-	movaps	xmm5, [f_0010]
-	movaps	xmm6, [f_2222]
+	movups	xmm5, [f_0010]
+	movups	xmm6, [f_2222]
 	align	16
     .0:
-	movaps	xmm1, [eax]		;    ux     |    uy     |    uz     |    uw
-	movaps	xmm2, [ecx]		;    nx     |    ny     |    nz     |    nw
-	movaps	xmm0, xmm1		;    ux     |    uy     |    uz     |    uw
+	movups	xmm1, [eax]		;    ux     |    uy     |    uz     |    uw
+	movups	xmm2, [ecx]		;    nx     |    ny     |    nz     |    nw
+	movups	xmm0, xmm1		;    ux     |    uy     |    uz     |    uw
 	mulps	xmm1, xmm2		;   ux*nx   |   uy*ny   |   uz*nz   |   uw*nw
-	movaps	xmm3, xmm1		;   ux*nx   |   uy*ny   |   uz*nz   |   uw*nw
-	movaps	xmm4, xmm1		;   ux*nx   |   uy*ny   |   uz*nz   |   uw*nw
+	movups	xmm3, xmm1		;   ux*nx   |   uy*ny   |   uz*nz   |   uw*nw
+	movups	xmm4, xmm1		;   ux*nx   |   uy*ny   |   uz*nz   |   uw*nw
 	shufps	xmm3, xmm3, SHUF(Y, Z, X, W)
 	shufps	xmm4, xmm4, SHUF(Z, X, Y, W)
 	addps	xmm1, xmm3
@@ -246,7 +246,7 @@ proc	sse_reflect
 	mulps	xmm1, xmm2
 	subps	xmm1, xmm5
 	subps	xmm0, xmm1
-	movaps	[edx], xmm0
+	movups	[edx], xmm0
 
 	add	eax, 16
 	add	ecx, 16
@@ -265,17 +265,17 @@ proc	sse_reflect_mvec
 	mov	eax, [tnl_vb + TNL_VB_VEYN]
 	mov	ecx, [tnl_vb + TNL_VB_NEYE]
 	mov	edx, [tnl_vb + TNL_VB_REFL]
-	movaps	xmm5, [f_0010]
-	movaps	xmm6, [f_2222]
-	movaps	xmm7, [f_h000]
+	movups	xmm5, [f_0010]
+	movups	xmm6, [f_2222]
+	movups	xmm7, [f_h000]
 	align	16
     .0:
-	movaps	xmm1, [eax]		;    ux     |    uy     |    uz     |    uw
-	movaps	xmm2, [ecx]		;    nx     |    ny     |    nz     |    nw
-	movaps	xmm0, xmm1		;    ux     |    uy     |    uz     |    uw
+	movups	xmm1, [eax]		;    ux     |    uy     |    uz     |    uw
+	movups	xmm2, [ecx]		;    nx     |    ny     |    nz     |    nw
+	movups	xmm0, xmm1		;    ux     |    uy     |    uz     |    uw
 	mulps	xmm1, xmm2		;   ux*nx   |   uy*ny   |   uz*nz   |   uw*nw
-	movaps	xmm3, xmm1		;   ux*nx   |   uy*ny   |   uz*nz   |   uw*nw
-	movaps	xmm4, xmm1		;   ux*nx   |   uy*ny   |   uz*nz   |   uw*nw
+	movups	xmm3, xmm1		;   ux*nx   |   uy*ny   |   uz*nz   |   uw*nw
+	movups	xmm4, xmm1		;   ux*nx   |   uy*ny   |   uz*nz   |   uw*nw
 	shufps	xmm3, xmm3, SHUF(Y, Z, X, W)
 	shufps	xmm4, xmm4, SHUF(Z, X, Y, W)
 	addps	xmm1, xmm3
@@ -286,10 +286,10 @@ proc	sse_reflect_mvec
 	subps	xmm1, xmm5
 	subps	xmm0, xmm1
 
-	movaps	xmm1, xmm0
+	movups	xmm1, xmm0
 	mulps	xmm1, xmm1
-	movaps	xmm2, xmm1
-	movaps	xmm3, xmm1
+	movups	xmm2, xmm1
+	movups	xmm3, xmm1
 	shufps	xmm2, xmm2, SHUF(Y, Z, X, W)
 	shufps	xmm3, xmm3, SHUF(Z, X, Y, W)
 	addps	xmm1, xmm2
@@ -298,7 +298,7 @@ proc	sse_reflect_mvec
 	mulps	xmm1, xmm7
 
 	movt	xmm0, xmm1
-	movaps	[edx], xmm0
+	movups	[edx], xmm0
 
 	add	eax, 16
 	add	ecx, 16
diff --git sage/x86/sse_vertex.asm sage/x86/sse_vertex.asm
index e09b163..48aa5a2 100644
--- sage/x86/sse_vertex.asm
+++ sage/x86/sse_vertex.asm
@@ -28,7 +28,7 @@ proc	sse_Vertex3fv, 4
 	shufps	xmm0, [f_0001], SHUF(X, Y, Z, W)
 	shufps	xmm0, xmm0, SHUF(Y, Z, X, W)
 	movlps	xmm0, [eax]
-	movaps	[edx], xmm0
+	movups	[edx], xmm0
 	cmp	ecx, [tnl_vb + TNL_VB_MAX]
 	je	.0
 	ret
@@ -50,7 +50,7 @@ proc	sse_Color3fv, 4
 	shufps	xmm0, [f_0001], SHUF(X, Y, Z, W)
 	shufps	xmm0, xmm0, SHUF(Y, Z, X, W)
 	movlps	xmm0, [edx]
-	movaps	[eax], xmm0
+	movups	[eax], xmm0
 	mov	eax, [tnl_vb + TNL_VB_FLAGS]
 	or	dword [eax + ecx * 4], TNL_COLOR0_BIT
 	or	dword [tnl_general_flags], TNL_COLOR0_BIT
@@ -66,7 +66,7 @@ proc	sse_Color4fv, 4
 	sal	eax, 4
 	add	eax, [tnl_vb + TNL_VB_COLOR0_PTR]
 	movups	xmm0, [edx]
-	movaps	[eax], xmm0
+	movups	[eax], xmm0
 	mov	eax, [tnl_vb + TNL_VB_FLAGS]
 	or	dword [eax + ecx * 4], TNL_COLOR0_BIT
 	or	dword [tnl_general_flags], TNL_COLOR0_BIT
@@ -81,9 +81,9 @@ proc	sse_TexCoord2fv, 4
 	mov	eax, ecx
 	sal	eax, 4
 	add	eax, [tnl_vb + TNL_VB_TEXCOORD0_PTR]
-	movaps	xmm0, [f_0001]
+	movups	xmm0, [f_0001]
 	movlps	xmm0, [edx]
-	movaps	[eax], xmm0
+	movups	[eax], xmm0
 	mov	eax, [tnl_vb + TNL_VB_FLAGS]
 	or	dword [eax + ecx * 4], TNL_TEXCOORD0_BIT
 	or	dword [tnl_general_flags], TNL_TEXCOORD0_BIT
@@ -102,9 +102,9 @@ proc	sse_MultiTexCoord2fv, 8
 	mov	eax, ebx
 	sal	eax, 4
 	add	eax, [tnl_vb + edx * 4 - GL_TEXTURE0 * sizeof_TNL_ARRAY + TNL_VB_TEXCOORD0_PTR]
-	movaps	xmm0, [f_0001]
+	movups	xmm0, [f_0001]
 	movlps	xmm0, [esi]
-	movaps	[eax], xmm0
+	movups	[eax], xmm0
 	mov	edx, [tnl_vb + TNL_VB_FLAGS]
 	sub	ecx, GL_TEXTURE0
 	mov	eax, TNL_TEXCOORD0_BIT
